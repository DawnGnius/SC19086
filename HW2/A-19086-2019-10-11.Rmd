---
title: "Homework 3"
author: "By 19086"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1111)
```

## Question 1 (5.1)
Compute a Monte Carlo estimate of 
\begin{equation}
  \int_0^{\pi/3} sin t dt
\end{equation}
and compare your estimate with the exact value of the integral.

## Question 2 (5.10)
Use Monte Carlo integration with antithetic variables to estimate
\begin{equation}
  \int_0^1 \frac{e^{-x}}{1+x^2} dx,
\end{equation}
and find the approximate reduction in variance as a percentage of the variance without variance reduction.

## Question 3 (5.15)
Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10

## Answer 1
we can replace the $\text{Uniform}(0,1)$ density with $\text{Uniform}(0,\pi/3)$.
```{r a1}
m <- 10000
x <- runif(m, min=0, max=pi/3)
theta.hat <- mean(sin(x)) *  (pi/3)
theta.real <- cos(0) - cos(pi/3)
cat("Simple Monte Carlo integration:\t", theta.hat, "\nExact value of the integral:\t\t", theta.real, "\n")
```

## Answer 2
The target parameter is $\theta=E_U [e^x/(1+x^2)]$ where $U$ has the $\text{Uniform}(0,1)$ distribution. $g(x)= e^x/(1+x^2)$ is monotone, so the hypothesis of Corollary 5.1 is satisfied. 
Generate random numbers $u_1, . . . , u_{m/2} \sim \text{Uniform}(0,1)$ and compute half of the replicates using
\begin{equation}
  Y_j = g^{(j)}(u) = e^{-u_j}/(1+u_j^2), \quad\quad j=1,\dots,m/2
\end{equation}
the remaining half of the replicates using
\begin{equation}
  Y'_j = g^{(j)}(1-u) = e^{-1+u_j}/(1+(1-u_j)^2), \quad\quad j=1,\dots,m/.
\end{equation}

The sample mean 
\begin{equation}
  \begin{split}
    \hat{\theta} &  = \overline{g_m(u)} = \frac{1}{m} \sum_{j=1}^{m/2} (Y_j  + Y'_j ) \\
    & = \frac{1}{m/2} \sum_{j=1}^{m/2} (\frac{Y_j / 2 + Y'_j / 2}{2})
  \end{split}
\end{equation}
converges to $E[\hat{\theta}] = \theta$ as $m \to \infty$.

The Monte Carlo estimation of the integral is implemented below. 

```{r a2}
MC.theta <- function(R=10000, antithetic=TRUE){
  u <- runif(R/2)
  if(!antithetic){
    v <- runif(R/2)
  } else {
    v <- 1 - u  
  }
  u <- c(u, v)
  g <- exp(-u) / (1+u^2)
  theta <- mean(g)
  theta
}
```

A comparison of estimates obtained from a single Monte Carlo experiment is below.
```{r a2cont1}
g <- function(x){
  exp(-x) / (1+x^2)
}
MC1 <- MC.theta(antithetic=FALSE)
MC2 <- MC.theta()
Theta <- integrate(g, 0, 1)$value
print(round(cbind(Theta, MC1, MC2), 5))
```
The approximate reduction in variance 
```{r a2cont2}
m <- 1000
MC1 <- MC2 <- numeric(m)
for (i in 1:m) {
  MC1[i] <- MC.theta(R=1000, anti=FALSE)
  MC2[i] <- MC.theta(R=1000)
}
sdMC1 <- sd(MC1)
sdMC2 <- sd(MC2)
Reduction <- (var(MC1) - var(MC2))/var(MC1)
print(round(cbind(sdMC1, sdMC2, Reduction), 5))
```
The antithetic variable approach achieved approximately $96.459\%$ reduction in variance.

## Answer 3

### Estimation
We divide the interval $(0,1)$ into five subintervals, $(a_j, a_{j+1}), j = 0, 1, . . . , 4$ where $a_i$ is $F^{-1}(j/5),\;j=1,2,3,4$, $a_0=1-a_5=0$, $F(x) = \frac{1-e^{-x}}{1-e^{-1}}$.
Then on the $j$th subinterval variables are generated from the density
\begin{equation}
  \frac{5e^{-x}}{1-e^{-1}},\quad\quad a_{j}<x<a_{j+1}
\end{equation}

The integrand is 
\begin{equation}
  g(x) = \left\{
  \begin{split}
    e^{-x}/(1+x^2),& \text{ if }(0<x<1); \\
    0,\quad\quad\quad\quad\quad & \text{ otherwise}.
  \end{split}
  \right.
\end{equation}

What we wanna is the expections on each interval
\begin{equation}
  E \left(\frac{1-e^{-1}}{5\left(1+X^{2}\right)}\right)
\end{equation}
where $X$ is obtained by density above.

```{r a3}
m <- 10000
theta.hat <- se <- numeric(6)
g <- function(x) {
  exp(-x - log(1+x^2)) * (x > 0) * (x < 1)
}
x <- runif(m) # using f0
fg <- g(x)
theta.hat[1] <- mean(fg)
se[1] <- sd(fg)
x <- rexp(m, 1) # using f1
fg <- g(x) / exp(-x)
theta.hat[2] <- mean(fg)
se[2] <- sd(fg)
x <- rcauchy(m) # using f2
i <- c(which(x > 1), which(x < 0))
x[i] <- 2 # to catch overflow errors in g(x)
fg <- g(x) / dcauchy(x)
theta.hat[3] <- mean(fg)
se[3] <- sd(fg)
u <- runif(m) # f3, inverse transform method
x <- - log(1 - u * (1 - exp(-1)))
fg <- g(x) / (exp(-x) / (1 - exp(-1)))
theta.hat[4] <- mean(fg)
se[4] <- sd(fg)
u <- runif(m) # f4, inverse transform method
x <- tan(pi * u / 4)
fg <- g(x) / (4 / ((1 + x^2) * pi))
theta.hat[5] <- mean(fg)
se[5] <- sd(fg)

F.inv <- function(x){
  # (1-exp(-x))/(1-exp(-1))
  -log(1-(1-exp(-1))*x)
}
a <- c(0,F.inv(0.2),F.inv(0.4),F.inv(0.6),F.inv(0.8),1)

M <- 20 # number of replicates
T2 <- numeric(5)
n <- 100  # 100 times
estimates <- matrix(0, n, 2)
g <- function(x) {
  exp(-x - log(1+x^2))# * (x > 0) * (x < 1) 
}

for (i in 1:n) {
  estimates[i, 1] <- mean(g(runif(M))) # simple MCMC
  for (jj in 1:5){
    u <- runif(m) # f3, inverse transform method
    x <- - log(exp(-a[jj]) - u * (1 - exp(-1)) / 5)
    fg <- g(x) / (5*exp(-x) / (1 - exp(-1))) # the function of the random variable, we want to calculate its expectation
    T2[jj] <- mean(fg)
  }
  estimates[i, 2] <- sum(T2)
}

theta.hat[6] <- mean(estimates[,2])
se[6] <- sd(estimates[,2])
```
Comparation with example from EX5.10, where the first 5 colums are result from EX5.10 with different density function $f$ and the 6th colum is result from **Stratified Importance sampling**.
```{r}
print(rbind(theta.hat, se))
```

Some estimation results:
```{r}
print(estimates[,2])
```


From the result above we can see that Stratified Importance sampling has the smallest **Std. Error** of estimation as well as **accurate** results.
