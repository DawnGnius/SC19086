---
title: "Homework 3"
author: "By 19086"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(1111)
```

## Question 1 (5.1)
Compute a Monte Carlo estimate of 
\begin{equation}
  \int_0^{\pi/3} sin t dt
\end{equation}
and compare your estimate with the exact value of the integral.

## Question 2 (5.10)
Use Monte Carlo integration with antithetic variables to estimate
\begin{equation}
  \int_0^1 \frac{e^{-x}}{1+x^2} dx,
\end{equation}
and find the approximate reduction in variance as a percentage of the variance without variance reduction.

## Question 3 (5.15)
Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10

## Answer 1
we can replace the $\text{Uniform}(0,1)$ density with $\text{Uniform}(0,\pi/3)$.
```{r a1}
m <- 10000
x <- runif(m, min=0, max=pi/3)
theta.hat <- mean(sin(x)) *  (pi/3)
theta.real <- cos(0) - cos(pi/3)
cat("Simple Monte Carlo integration:\t", theta.hat, "\nExact value of the integral:\t\t", theta.real, "\n")
```

## Answer 2
The target parameter is $\theta=E_U [e^x/(1+x^2)]$ where $U$ has the $\text{Uniform}(0,1)$ distribution. $g(x)= e^x/(1+x^2)$ is monotone, so the hypothesis of Corollary 5.1 is satisfied. 
Generate random numbers $u_1, . . . , u_{m/2} \sim \text{Uniform}(0,1)$ and compute half of the replicates using
\begin{equation}
  Y_j = g^{(j)}(u) = e^{-u_j}/(1+u_j^2), \quad\quad j=1,\dots,m/2
\end{equation}
the remaining half of the replicates using
\begin{equation}
  Y'_j = g^{(j)}(1-u) = e^{-1+u_j}/(1+(1-u_j)^2), \quad\quad j=1,\dots,m/.
\end{equation}

The sample mean 
\begin{equation}
  \begin{split}
    \hat{\theta} &  = \overline{g_m(u)} = \frac{1}{m} \sum_{j=1}^{m/2} (Y_j  + Y'_j ) \\
    & = \frac{1}{m/2} \sum_{j=1}^{m/2} (\frac{Y_j / 2 + Y'_j / 2}{2})
  \end{split}
\end{equation}
converges to $E[\hat{\theta}] = \theta$ as $m \to \infty$.

The Monte Carlo estimation of the integral is implemented below. 

```{r a2}
MC.theta <- function(R=10000, antithetic=TRUE){
  u <- runif(R/2)
  if(!antithetic){
    v <- runif(R/2)
  } else {
    v <- 1 - u  
  }
  u <- c(u, v)
  g <- exp(-u) / (1+u^2)
  theta <- mean(g)
  theta
}
```

A comparison of estimates obtained from a single Monte Carlo experiment is below.
```{r a2cont1}
g <- function(x){
  exp(-x) / (1+x^2)
}
MC1 <- MC.theta(antithetic=FALSE)
MC2 <- MC.theta()
Theta <- integrate(g, 0, 1)$value
print(round(cbind(Theta, MC1, MC2), 5))
```
The approximate reduction in variance 
```{r a2cont2}
m <- 1000
MC1 <- MC2 <- numeric(m)
for (i in 1:m) {
  MC1[i] <- MC.theta(R=1000, anti=FALSE)
  MC2[i] <- MC.theta(R=1000)
}
sdMC1 <- sd(MC1)
sdMC2 <- sd(MC2)
Reduction <- (var(MC1) - var(MC2))/var(MC1)
print(round(cbind(sdMC1, sdMC2, Reduction), 5))
```
The antithetic variable approach achieved approximately $96.459\%$ reduction in variance.

## Answer 3

