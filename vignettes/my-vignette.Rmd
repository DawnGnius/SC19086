---
title: "my-vignette"
author: Huihang Liu
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(StatComp19086)
```

# Project one: FDP Estimation

## Experiments

In this simulation studies, we consider $p = 2000$, $n = 100$, $\sigma = 2$, the number of false null hypotheses $p_1 = 10$, and the nonzero $\beta_i = 1$, unless stated otherwise. 
We will present six different dependence structures for $\Sigma$ of the test statistics $(Z_1,\dots, Z_p)T \sim N((\mu_1, \dots , \mu_p)^T ,\Sigma)$. 
$\Sigma$ is the correlation matrix of a random sample of size $n$ of $p$-dimensional vector $\mathbf{X}_i = (X_{i1}, \dots , X_{ip})$, and $\mu_j = \sqrt{n} \beta_j \hat{\sigma}_j / \sigma, j = 1, \dots , p$. 
The data-generating process vector $X_i$’s are as follows.

\begin{description}
    \item [Equal correlation] Let $\mathbf{X}^T = (X_{1}, \dots , X_{p})^T \sim N_p(0, \Sigma)$, where $\Sigma$ has diagonal element $1$ and off-diagonal element $1/2$.

    \item [Fan and Song's model] For $\mathbf{X} = (X_{1}, \dots , X_{p})$, let $\{X_k\}^{1900}_{k=1}$ be iid $N(0,1)$ and
    \begin{equation}
        X_{k}=\sum_{l=1}^{10} X_{l}(-1)^{l+1} / 5+\sqrt{1-\frac{10}{25}} \epsilon_{k}, k=1901, \ldots, 2000
    \end{equation}
    where $\{\epsilon_{k}\}^{2000}_{k=1901}$ are standard normally distributed.

    \item [Independent Cauchy] For $\mathbf{X} = (X_{1}, \dots , X_{p})$, let $\{X_k\}^{2000}_{k=1}$ be iid. Cauchy random variables with location parameter $0$ and scale parameter $1$.

    \item [Three factor model] For $\mathbf{X} = (X_{1}, \dots , X_{p})$, let 
    \begin{equation}
        X_{j}=\rho_{j}^{(1)} W^{(1)}+\rho_{j}^{(2)} W^{(2)}+H_{j}
    \end{equation}
    where $W^{(1)} \sim N(−2,1)$, $W^{(2)} \sim N(1,1)$, $W^{(3)} \sim N(4,1)$, $\rho^{(1)}_j$, $\rho^{(2)}_j$, $\rho^{(3)}_j$ are iid $U(−1,1)$, and $H_j$ are iid $N(0,1)$.

    \item [Two factor model] For $\mathbf{X} = (X_{1}, \dots , X_{p})$, let
    \begin{equation}
        X_{j}=\rho_{j}^{(1)} W^{(1)}+\rho_{j}^{(2)} W^{(2)}+H_{j}
    \end{equation}
    where $W^{(1)}$ and $W^{(2)}$ are iid $N(0,1)$, $\rho_j^{(1)}$ and $\rho_j^{(2)}$ are iid $U(−1,1)$, and $H_j$ are iid $N(0,1)$. 

    \item [Nonlinear factor model] For $\mathbf{X} = (X_{1}, \dots , X_{p})$, let
    \begin{equation}
        X_{j}=\sin \left(\rho_{j}^{(1)} W^{(1)}\right)+sgn\left(\rho_{j}^{(2)}\right) \exp \left(\left|\rho_{j}^{(2)}\right| W^{(2)}\right)+H_{j}
    \end{equation}
    where $W^{(1)}$ and $W^{(2)}$ are iid $N(0,1)$, $\rho_j^{(1)}$ and $\rho_j^{(2)}$ are iid $U(−1,1)$, and $H_j$ are iid $N(0,1)$. 
\end{description}

## Code and results
In the following, I try to repeat the result in simulation 1 and theorem 1 of paper. 
I use the same setting in the following as described in the paper. 

To get the distribution of $FDR$ and $\widehat{FDR}$, I generate $X\sim N(0, \Sigma)$ in Equal correlation structure. 
Then calcuate $\mathbf{Z}$ by $Z_i = \frac{\hat{\beta}_i}{\sigma/(\sqrt{n}\hat{\sigma})}$. 

By equation (10) in the paper, we can write $Z_i$ as 
\begin{equation}
  Z_i = \mu_i + \mathbf{b}_i^T \mathbf{W} + K_k, \quad i=1,\dots,p.
\end{equation}

To calculate $\mathbf{W}$, I use the L1 regression as following 
\begin{equation}
  \widehat{\mathbf{w}} \equiv \operatorname{argmin}_{\beta \in R^{k}} \sum_{i=1}^{m}\left|Z_{i}-\mathbf{b}_{i}^{T} \beta\right|
\end{equation}
which is robust. 
And L1 regression is done by *l1fit* defined in package **L1pack**.

```{r, eval=TRUE}
library(MASS)
library(snowfall)
library(ggplot2)
library(L1pack)

set.seed(12345)

n <- 100; rho <- 0.5; sig <- 2; p.nonzero <- 10; beta.nonzero <- 1

# FDP and FDP_lim at t
my.fdp <- function(t){
  ##  FDP
  Z <- MASS::mvrnorm(1, mu, Sigma)
  pvalue <- unlist(base::lapply(X=1:p, FUN=function(ii) 1-pnorm(abs(Z[ii]))))
  tmp.pvalue <- pvalue[(1+p.nonzero):p]
  re1 <- length(which(tmp.pvalue < t)) / length(which(pvalue < t))
  
  ##  FDP_lim
  # k is dimension of W
  k <- 2
  # m.idx contains smallest 90% of |zi|’s indexes
  m.idx <- order(abs(Z), decreasing=TRUE)[(0.1*p+1):p]
  # x is the first k cols, eq(22)
  x.tmp <- (diag(sqrt(Sigma.eigen$values)) %*% Sigma.eigen$vectors)[m.idx, 1:k]
  y.tmp <- Z[m.idx]
  # L1 regression by eq(23)
  W <- L1pack::l1fit(x=x.tmp, y=y.tmp, intercept=FALSE)$coefficients
  # b is given by eq(22)
  b <- diag(sqrt(Sigma.eigen$values)) %*% Sigma.eigen$vectors[, 1:k]
  # numerator is given by eq(12)
  numerator <- sum(unlist(base::lapply(X=1:p.nonzero, FUN=function(ii) {
    ai <- (1 - sum((b[ii, ])^2))^(-0.5)
    pnorm(ai*(qnorm(t/2) + b[ii,] %*% W)) + pnorm(ai*(qnorm(t/2) - 
                                                        b[ii,] %*% W))})))
  # eq(12)
  denominator <- sum(unlist(base::lapply(X=1:p, FUN=function(ii) {
    ai <- (1 - sum((b[ii, ])^2))^(-0.5)
    pnorm(ai*(qnorm(t/2) + b[ii,] %*% W + mu[ii])) + 
      pnorm(ai*(qnorm(t/2) - b[ii,] %*% W - mu[ii]))})))
  # eq(12)
  re2 <- numerator / denominator
  return(rbind(re1, re2))
}

my.plot <- function(p, t){
  # Equal correlation
  beta <- c(rep(beta.nonzero, p.nonzero), rep(0, p-p.nonzero))
  Sigma <- matrix(rep(rho, p*p), p, p); diag(Sigma) <- rep(1, p)
  dat <- MASS::mvrnorm(n, rep(0,p), Sigma)
  Sigma.eigen <- eigen(Sigma)
  mu <- unlist(base::lapply(X=1:p, FUN=function(ii) 
    sqrt(n)*beta[ii]*sqrt(var(dat[, ii]))/sig))
  
  # parallel calculation
  snowfall::sfInit(parallel = TRUE, cpus = 3)
  snowfall::sfLibrary(MASS)
  snowfall::sfLibrary(L1pack)
  snowfall::sfExport("p", "mu", "Sigma", "t", "p.nonzero", "Sigma.eigen", "rho")
  fdp.repeat <- unlist(snowfall::sfLapply(rep(0.01, 100), my.fdp))
  snowfall::sfStop()
  
  # figure
  tmp.data <- data.frame(fdp=fdp.repeat[(1:p)*2-1])
  tmp.data.lim <- data.frame(fdp=fdp.repeat[(1:p)*2])
  pic <- ggplot()
  pic <- pic + geom_histogram(data=tmp.data, aes(fdp, y=..density..), bins=5, 
                              color=1, alpha=0.1) 
  pic <- pic + geom_histogram(data=tmp.data.lim, aes(fdp, y=..density..), 
                              bins=5, color=2, alpha=0.5)
  pic <- pic + ggtitle(paste("FDP with p=", p, "t=", t, sep=' '))
  plot(pic)
}

my.plot(p=100, t=0.01)
# my.fun(p=500, t=0.01)
# my.fun(p=100, t=0.005)
# my.fun(p=500, t=0.005)
```

The grey bars in the figures is the density of $FDR$ and the red bars in the figures represent the density of $\widehat{FDR}$.

From the figures above, I find the true $FDR$ is very similiar with the result in the paper. 
But $\widehat{FDR}$ are not similar with $FDR$. 

I suppose it is because my $\hat{W}$ is some kind of incorrect, maybe. But the paper just show the result of two factor model.

# Project two: High Dimensional Nonparameter Estimation

## One-Dimensional Example

We are going to estimate the density of simulated one-dim data using classical DE methods and proposed method.

### Basic KDE methods
```{r}
# data preparation
set.seed(111)
n <- 200
Num.Cmp <- 8
pro <- rep(1/8, Num.Cmp)
multi <- sample(1:Num.Cmp, n, replace = T, prob=pro)
mu <- 3 * ((2/3)^(1:Num.Cmp) - 1)
sigma <- (2/3)^(1:Num.Cmp)
x <- NULL
for (ii in 1:Num.Cmp) {
  com_txt <- paste("com", ii, " <- rnorm(length(which(multi==", ii, ")), mean=", mu[ii], ", sd=", sigma[ii], ")",sep="")
  eval(parse(text=com_txt))
  com_txt <- paste("x <- c(x, com", ii, ")", sep="")
  eval(parse(text=com_txt))
}

# true density function, y is h, and z is v.
y <- seq(-3, 1, 0.01)
z <- rep(0, length(y))
for (ii in 1:Num.Cmp) {
  z <- z + pro[ii] * dnorm(y, mean=mu[ii], sd=sigma[ii])
}
```

### Rodeo algorithm
```{r}
# Rodeo local for 1-dim
rodeo.local.bw1 <- function(xx, x, h.init=1.3/log(log(n)), beta=0.9, cn=log(n)/n){
  # bandwidth selection
  # para: xx        target point at which we want to estimate f(x)
  # para: x         samples
  # para: beta      learning rate
  # value: h.init   bandwidth
  h1 <- h.init
  while(TRUE){
    Z.i <- ((xx - x)^2 - h1^2) * exp(- (xx - x)^2 / h1^2 / 2) / h1^4 / sqrt(2*pi)
    Z <- mean(Z.i) 
    s <- var(Z.i)
    lam <- sqrt(2*s*log(n*cn)) / 10
    if (abs(Z) > lam) {
      h1 <- h1 * beta
    } else {
      return(h1)
    }
  }
}

rodeo.local1 <- function(t, x){
  # estimate target points t
  # para: t   target points, a vector
  # para: x   data points, a vector
  h <- unlist(base::lapply(X=t, FUN=rodeo.local.bw1, x=x))
  K <- stats::dnorm
  f.hat <- unlist(base::lapply(X=1:length(t), FUN=function(ii) mean(K((t[ii] - x) / h[ii]))/ h[ii]))
  return(f.hat)
}

# plot h and density
t <- seq(-3, 1, 0.05)
h <- unlist(base::lapply(X=t, FUN=rodeo.local.bw1, x=x))
plot(t, h, "l", main="Bandwidth of Rodeo", xlab="x", ylab="h")
fit.rodeo <- rodeo.local1(t=t, x=x)
plot(t, fit.rodeo, "l", lty=2, ylim=c(0,2.5), xlim=c(-3, 1), main="Rodeo", xlab="x", ylab="Density")
lines(y, z, lty=1, lwd=2)
legend("topright", legend=c("True Density", "Rodeo"), col=1, lty=c(1, 2), lwd=c(2, 1))
```


## Two-Dimensional Example

We evaluate a simulated dataset and a real dataset. 
The density rodeo’s performance is compared with the built-in method *KDE2d* from the *MASS* package in R. 

### Mixture of Beta distributions, with the uniform distribution for an irrelevant dimension

Data
```{r}
# data preparation
set.seed(13245)

n <- 2000
Num.Cmp <- 2
pro <- c(2/3, 1/3)
multi <- sample(1:Num.Cmp, n, replace = T, prob=pro)
shape1 <- c(1, 10)
shape2 <- c(2, 10)
x1 <- NULL
for (ii in 1:Num.Cmp) {
  com_txt <- paste("com", ii, " <- rbeta(length(which(multi==", ii, ")), shape1=", shape1[ii], ", shape2=", shape2[ii], ")",sep="")
  eval(parse(text=com_txt))
  com_txt <- paste("x1 <- c(x1, com", ii, ")", sep="")
  eval(parse(text=com_txt))
}
x2 <- runif(n, 0, 1)
samples <- cbind(x1, x2)

p <- dim(samples)[2]
n <- dim(samples)[1]

# true density of first dimension
pro <- c(2/3, 1/3)
shape1 <- c(1, 10)
shape2 <- c(2, 10)
y <- seq(0, 1, 0.01)
z <- rep(0, length(y))
for (ii in 1:2) {
  z <- z + pro[ii] * dbeta(y, shape1[ii], shape2[ii])
}
```

KDE2D esitmation
```{r, warning=FALSE, message=FALSE}
# KDE2D esitmation
library(MASS)
fit.kde2d <- MASS::kde2d(x1, x2)

# draw figures
library(plot3D)
xx <- as.matrix(fit.kde2d$x)
yy <- as.matrix(fit.kde2d$y)
zz <- as.matrix(fit.kde2d$z)

tmp <- plot3D::mesh(xx, yy)
xx <- tmp$x
yy <- tmp$y
surf3D(x=xx, y=yy, z=zz, xlab="relevant variable", ylab="irrelevant variable", zlab="Density", main="KDE2d", bty = "f", colkey=FALSE, colvar=matrix(rep(1, length(zz)), nrow(zz)), border = "black", theta=0)
```

### Main function of rodeo in 2 dim
```{r, warning=FALSE, message=FALSE}
# Rodeo lacal multivariable version
library(stats)

# for 2-dim
rodeo.local.bw2 <- function(xx, x, h.init=1/log(log(n)), beta=0.95, cn=log(n)/n){
  # bandwidth selection
  # para: xx        target point at which we want to estimate f(x), 2 dim vector
  # para: x         samples, n*2 matrix
  # para: beta      learning rate
  # value: h.hat    bandwidth selected, a 2 dim vector
  h.hat <- rep(h.init, 2)
  A <- 1:p
  while(length(A) > 0){
    for (jj in A) {
      Z.i <- ((xx[jj] - x[, jj])^2 - h.hat[jj]^2) * exp(- (xx[1] - x[,1])^2 / h.hat[1]^2 / 2) * exp(- (xx[2] - x[,2])^2 / h.hat[2]^2 / 2) / h.hat[jj]^3 / h.hat[1] / h.hat[2] / 2 / pi
      Z <- mean(Z.i) 
      s <- var(Z.i)
      lam <- sqrt(2*s*log(n*cn)) / 30
      if (abs(Z) > lam) {
        h.hat[jj] <- h.hat[jj] * beta
      } else {
        A <- A[A != jj]
      }
    }
  }
  return(h.hat)
}

rodeo.local2 <- function(t, x){
  # estimate target points t
  # para: t   target points, a matrix, each row is a point
  # para: x   data points, a matrix, each row is a point
  h <- t(base::apply(X=t, MARGIN=1, FUN=rodeo.local.bw2, x=x))
  K <- stats::dnorm
  f.hat <- unlist(base::lapply(X=1:dim(t)[1], FUN=function(ii) mean(K((t[ii, 1] - x[, 1]) / h[ii, 1]) * K((t[ii, 2] - x[, 2]) / h[ii, 2])) / h[ii, 1] / h[ii, 2]))
  return(list(f.hat=f.hat, h.hat=h))
}
```

```{r, echo=FALSE, warning=FALSE}
# using Rodeo to estimate f(t)

a.para <- 0.025
# square region
tmp.mesh <- plot3D::mesh(seq(0, 1, a.para), seq(0, 1, a.para))
xx <- tmp.mesh$x
yy <- tmp.mesh$y
t1 <- matrix(xx, ncol=1)
t2 <- matrix(yy, ncol=1)
t <- cbind(t1, t2)

# # estimate
# suppressWarnings(res <- rodeo.local(t, samples))
res <- rodeo.local2(t, samples)
fit.rodeo <- res$f.hat
h.hat <- res$h.hat

# draw figure
library(plot3D)
zz <- matrix(fit.rodeo, nrow=nrow(xx), byrow=TRUE)
surf3D(x=xx, y=yy, z=zz, xlab="relevant variable", ylab="irrelevant variable", zlab="Density", main="Rodeo", bty = "f", colkey = FALSE, colvar=matrix(rep(1, length(zz)), nrow(zz)), border = "black", theta=90)
```